{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据：词表到向量的转换函数\n",
    "# 创建实验样本\n",
    "def loadDataSet():\n",
    "    # 创建样本，每一行为每句话进行分词后的集合\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 类别标签列表，1 代表侮辱性, 0 代表非侮辱性\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatVocabList(dataSet):\n",
    "    # 创建词汇表(包含所有出现的单词且无重复的集合),初始为空\n",
    "    vocabSet = set([])\n",
    "    # 遍历数据集的每个实例\n",
    "    for document in dataSet:\n",
    "        # 求并集,即所有出现的单词\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    '''\n",
    "    输入为词汇表和一个实例向量(即一个分词后的句子向量)\n",
    "    '''\n",
    "    # 创造一个长度和词汇表长度相同的句子特征向量\n",
    "    returnVec = np.zeros((len(vocabList)))\n",
    "    # 对该句子向量，如果某个存在于词汇表中的词出现了，\n",
    "    # 就将句子特征向量中该词在词汇表中对应的索引位置的值设为1\n",
    "    for word in inputSet:\n",
    "        if(word in vocabList):\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"The word %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listOposts\n",
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "listClasses\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "['stop', 'help', 'park', 'dog', 'quit', 'please', 'stupid', 'posting', 'garbage', 'my', 'him', 'dalmation', 'steak', 'food', 'flea', 'ate', 'mr', 'problems', 'cute', 'take', 'maybe', 'has', 'love', 'not', 'licks', 'worthless', 'to', 'is', 'buying', 'I', 'so', 'how']\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "print('listOposts');print(listOPosts)\n",
    "print('listClasses');print(listClasses)\n",
    "myVocabList = creatVocabList(listOPosts)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    '''\n",
    "    trainMatrix 为文档集合矩阵(含n个文档,即n个单词向量)\n",
    "    trainCategory 为文档集合中的类别标签向量(含n个标签)\n",
    "    p(ci|w) = p(w|ci) * p(ci) / p(w)\n",
    "    函数返回 p(w|c1), p(w|c0),p(c1)\n",
    "    c1为侮辱性类别,c0为非侮辱性类别,w代表单词\n",
    "    '''\n",
    "    # numTrainDocs 为文档个数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # numWords 为一个文档的长度,也即单词表长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 出现侮辱性文档的概率 p(c1) = 所有文档中类别为1的比例\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 初始化概率\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    # 遍历所有文档\n",
    "    for i in range(numTrainDocs):\n",
    "        # 如果该文档是侮辱性\n",
    "        if(trainCategory[i] == 1):\n",
    "            # 统计所有类别为1的单词向量中各个单词出现的次数\n",
    "            # 若单词出现则在单词向量的位置为1,在p1Num该单词的位置 +1\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 统计类别为1所有文档中出现单词的数目\n",
    "            p1Denom += np.sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    # 计算p(wi|c1)\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    # 计算p(wi|c0)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.]), array([1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.]), array([0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "# 将文档集合从单词转变为one-hot编码\n",
    "for posinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, posinDoc))\n",
    "print(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -3.25809654 -3.25809654 -3.25809654 -1.87180218 -2.15948425 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "[-2.35137526 -3.04452244 -2.35137526 -1.94591015 -2.35137526 -3.04452244\n",
      " -1.65822808 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -1.94591015 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(p0V)\n",
    "print(p1V)\n",
    "print(pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    '''\n",
    "    vec2Classify是一个one-hot编码的文档,p0Vec和p1Vec是长度为单词表的向量,\n",
    "    p1Vec每个位置上的值为单词表中该位置的单词在类别为c1的文档中出现的概率\n",
    "    已知p(c1),p(wi|c1),则p(c1|wi) = p(wi|c1) * p(c1),用log将乘法转化为加法\n",
    "    左式忽略了p(wi),具体推导见《统计学习方法》第48页\n",
    "    '''\n",
    "    p1 = np.sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = np.sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if(p1 > p0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOposts, listClasses = loadDataSet()\n",
    "    myVocabList = creatVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,end=' ')\n",
    "    print('classified as:',end=' ')\n",
    "    print(classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,end=' ')\n",
    "    print('classified as:',end=' ')\n",
    "    print(classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = np.zeros(len(vocabList))\n",
    "    for word in inputSet:\n",
    "        if(word) in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: split() requires a non-empty pattern match.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
